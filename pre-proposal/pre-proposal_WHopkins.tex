\documentclass[letter, USenglish, 11pt, subfigure]{article}
\usepackage[margin=1in]{geometry}
\newcommand*{\ATLASLATEXPATH}{../}
\usepackage{\ATLASLATEXPATH atlaspackage}
\usepackage{\ATLASLATEXPATH atlasbiblatex}
\usepackage{\ATLASLATEXPATH atlasphysics}
% \usepackage{\ATLASLATEXPATH atlasjetetmiss}
\usepackage{\ATLASLATEXPATH ANA-SUSY-2018-12-PAPER-defs}
\usepackage{enumerate}
\newcommand{\mm}{\ensuremath{\mu^{+}\mu^{-}}}
\newcommand{\bsmm}{\ensuremath{\Bs\to\mm}}
\newcommand{\bdmm}{\ensuremath{\Bd\to\mm}}
\usepackage{lineno}
%\linenumbers
\usepackage{wrapfig}
\usepackage{placeins}
\usepackage{pdfpages}
\usepackage[none]{hyphenat} 
\addbibresource{../proposal_WHopkins.bib}
\addbibresource{../ATLAS-SUSY.bib}

\title{Early Career Pre-proposal: \\\input{../title}}
\author{Walter Hopkins, Assistant Physicist\\Argonne National Laboratory\\(630) 252 7551, whopkins@anl.gov\\Year Doctorate Awarded: 2013\\Eligibility Extension Requested: No\\Number of Times Previously Applied: 1\\ Topic Area: Experimental Research at the \\Energy Frontier in High Energy Physics\\
  FOA Number: DE-FOA-0002563\\Signature of Laboratory Director: \\ \\ \\Paul K. Kearns, Laboratory Director
}
\date{}
\begin{document}
\maketitle

Results from the Large Hadron Collider (LHC) experiments have verified the predictions of the highly successful Standard Model (SM). However, the SM  lacks an explanation for several observed phenomena (e.g., dark matter) motivating the search for Beyond the Standard Model (BSM) physics. The current BSM search strategy uses simplified BSM models (i.e., models with 2-5 parameters) to design BSM search regions. This simplified approach was driven by enhanced BSM physics sensitivity from the large increases of center-of-mass energies during early LHC upgrades. Future LHC upgrades will no longer include significant increases in energy, but will first result in a doubling (Run~3) and then a tenfold increase (High Luminosity-LHC, HL-LHC) of the current data set.

The HL-LHC comes with challenges; in particular, two linked challenges faced by the ATLAS experiment are (1) the need to exhaustively probe the large high-dimensional data set (where the dimensions are experimental observables such as particle momenta) for regions that might be hiding BSM physics and (2) the computational requirements of producing large amounts of simulations to estimate SM backgrounds in these potentially BSM enriched regions. This proposal presents {\bf the development of machine learning (ML) algorithms to both methodically search for BSM physics and enable accurate background simulations for these searches.} The upcoming increase in High Performance Computing (HPC) resources (such as the Aurora supercomputer, for which the PI is leading an early access proposal) and recent developments in ML yield a particularly well-timed and unique opportunity for facing these challenges.

The lack of evidence for BSM physics motivates a change in search strategy, specifically to move beyond using simplified theory models to design searches. Broader frameworks, e.g., the 19-parameter Phenomenological 
Minimal Supersymmetric Standard Model (pMSSM)~\cite{Djouadi_2007,Berger:2008cq,Cahill_Rowley_2012}, have been studied but have not been used to build a BSM search program. This is because they can produce thousands of models and applying the current labor-intensive search methodology at such a scale is not feasible. Previous attempts at categorizing the experimental observables of these models only included $\sim$10 models. This proposal presents {\bf the development of a novel search strategy for the HL-LHC based on probing the experimental observables of thousands of models with ML techniques.} The PI will develop the ML-driven search strategy in the context of ATLAS Supersymmetry (SUSY)
%~\cite{Golfand:1971iw,Volkov:1973ix,Wess:1974tw,Wess:1974jb,Ferrara:1974pu,Salam:1974ig}
searches, but the strategy could be applied to other models and experiments. The PI's experience in ML, starting with the use of ML for his Ph.D. thesis~\cite{Aaltonen:2011fi,Aaltonen:2013as},
will aid the success of this proposal. The PI will also draw from his work within the ATLAS SUSY group, both as a leader of flagship searches~\cite{stop0L_1,stopRun1,stop0L_2,stop0L_3} and as a subgroup convener of the ATLAS SUSY strong production group, to ensure the new search strategy is validated and ready to guide the physics search program for the HL-LHC. Additionally, the PI will leverage the computing resources and ML expertise at the Argonne Leadership Computing Facility (ALCF).

An important aspect of designing BSM search regions is the accurate estimation of SM backgrounds with simulations. ATLAS uses two frameworks for detector response simulations: a fast parameterization (FastSim) and an implementation of \GEANT (FullSim). The scale of the computational cost for the required SM background simulations at the HL-LHC prohibits the use of FullSim~\cite{computingCDR}, and FastSim has been shown to mismodel the decay products of heavy particles~\cite{hcw2018} which are likely to be present in BSM physic searches. Therefore, the PI {\bf proposes using ML as a way to utilize the significant computing power of HPCs to produce the needed ATLAS detector simulations}. This will be achieved by applying an ML-based correction to a modified configuration of FullSim, altered to be computationally faster. The use of an ML-based correction would allow for better use of HPC resources for simulation by performing the ML correction on graphical processing units (GPUs), which are expected to make up a large fraction of the computational power of future HPC resources, while running FullSim on the CPU resources. The success of the proposed work will be aided by the PI's experience with the ATLAS calorimeters and his work within the ATLAS \GEANT\ optimization task force, which studies computational bottlenecks of FullSim. 

%% \subsection*{Background and Motivation}

%% Identifying new promising regions in ATLAS's experimental observable space (the space made up of all experimental observables, such as momenta of particles and invariant masses of combinations of particles) is essential in developing future BSM searches.
%% The current methodology of searching for BSM physics involves manually optimizing the requirements on observables for each simplified BSM model with a particular choice of two to three theoretical parameters (e.g., the masses of two BSM particles that are within reach of the LHC). More complex BSM models, such as the 19-parameter pMSSM for the relationship between the pMSSM and simplified models), have been studied by both ATLAS~\cite{ATLAS_pMSSM} and CMS~\cite{CMS_pMSSM}. These studies involved scanning the 19 theory parameters, simulating the physics processes that result from a particular choice of parameters, applying simplified detector effects, and evaluating whether a particular set of theory model parameters was excluded by previous ATLAS or CMS searches. The non-excluded models were studied further to understand why they evaded previous searches. Within ATLAS, some of these models had their mass spectra manually inspected to gain insight into their expected signature in experimental observable space. This method resulted in some additional interpretation of standard early Run~2 SUSY searches~\cite{stopEarlyRun2}. However, the experimental space of the non-excluded models was not systematically probed and no new searches were designed on the basis of the non-excluded models due to the difficulty of interpreting thousands of models manually. CMS attempted to interpret the non-excluded models as a function of average values of observables. This figure yields valuable information about the observables of non-excluded models, but is difficult to interpret because of the high dimensionality of the observable space.


%% Systematically probing the observable space of non-excluded models, which could be harboring new physics, requires novel techniques that are able to summarize high-dimensional spaces. Machine learning is especially adept at processing high-dimensional spaces and can be used to identify promising new search regions inspired by broad theory models such as the pMSSM. In particular, ML clustering algorithms, which group similar data points together, could group thousands of pMSSM models with different theory parameters but similar experimental signatures, forming the basis for new search regions. Considering the large number of models, developing search regions on the basis of these models is simply not practical with previous methods. Additionally, ML's ability to derive high-level features, such as particle masses from daughter particle momenta, makes it an attractive approach for automatically designing new search regions using common low-level features. Thus ML has the potential to significantly expand BSM searches for the HL-LHC by guiding the searches in an automated and methodical way.

%% Owing to the scale of both the simulations needed for the clustering and the clustering itself, this proposal will take advantage of advanced computing resources such as HPC resources. Argonne National Laboratory has both computing resources (e.g., Aurora) and expertise (at the ALCF) that will make the realization of this proposal possible. Significant computing resources will be required to cluster the samples from the large pMSSM parameter scan. Additionally, the determination of the type of clustering algorithm and the optimal parameters for the clustering algorithm may require several passes through this large data set. The PI's team, consisting of two postdocs and himself, will build on the existing relationship with their ALCF colleagues to scale and optimize to both develop and optimize clustering algorithms.

%% BSM physics, that could be hiding in regions identified by clustering, is likely to include W/Z/Higgs boson or top quark decays, which can be mimicked by SM processes. These decays can be identified as jets (objects that represent the products of quark and gluon hadronization) with particular correlations between the jet constituents (substructure). Thus, for BSM physics searches, the modeling of substructure is crucial in estimating SM backgrounds accurately.

%% The parametric FastSim has been shown to mismodel the number of constituents of jets, strongly affecting the modeling of substructure. The effect of this mismodeling on the efficiency for identifying top quarks, which compares the efficiency in data, FastSim, and FullSim. There have been recent efforts to develop ML simulation, based on generative adversarial networks (GAN) and variational autoencoders, that would completely replace FastSim ~\cite{calogan,atlasgan}. These efforts show promise and actually improve the modeling of heavy particle decays somewhat. However, challenges remain in modeling as well as in aspects such as training (GANs are notoriously difficult to train) and interpretability. Thus, to maximize the discovery potential for BSM physics at the HL-LHC, some use of FullSim to simulate SM background may be required. 

%% However, producing simulations with FullSim for HL-LHC conditions with sufficient statistics is computationally prohibitive. Significant processing speed improvements to FullSim are required to meet budget constraints ~\cite{computingCDR}. One of the main computationally expensive parts of the ATLAS simulation is the simulation of particles navigating through the complex geometry of the EM calorimeters. Computational speed improvements can be achieved by eliminating particles (and thus their daughters) by tuning FullSim configurations. The reduction in simulated particles would remove many of the geometric calculations and save significant amounts of computing time.

%% The gains in computational speed from changing FullSim configurations typically reduce the accuracy of the simulation. This proposal aims to develop an ML-based correction to a faster FullSim configuration to ensure a minimal loss in accuracy. The ML-based correction algorithm can be run on a variety of computing resources such as future HPC resources. These resources will have significant fractions of compute power in the form of GPUs. Utilizing GPUs typically requires rewriting code, such as \GEANT, but an ML algorithm can exploit GPUs since ML libraries (such as TensorFlow~\cite{tensorflow2015-whitepaper} and PyTorch~\cite{NEURIPS2019_9015}) are already optimized for a variety of computing resources. Thus, the proposed work could enable the use of CPU and GPU computing resources for simulation (currently FullSim is only utilizing CPU resources), potentially allowing for the needed amount of FullSim for new physics searches at the HL-LHC.


\subsection*{Project Objectives}
The objective of the proposed work is to develop ML algorithms (i.e., clustering) to methodically design a BSM search strategy for the HL-LHC. The ongoing ATLAS Run~2 pMSSM scan, which is expected to conclude near the end of 2022 or early 2023, yields an opportunity to test prototype ML algorithms to derive a BSM physics search program for Run~3. To enable these searches for the HL-LHC, the proposed research also aims to prepare FullSim for HL-LHC conditions by preparing FullSim for use on future HPC resources and improving its computational speed with ML. The specific objectives are as follows:
\begin{itemize}
\item Develop an ML-driven BSM physics search strategy.
  \begin{itemize}
  \item Apply clustering to non-excluded models resulting from the ATLAS Run~2 pMSSM scan and identify promising regions for further development into new search regions for Run~3. The clustering algorithm's inputs will be human-developed high-level variables. 
  \item Introduce automatic high-level variable (e.g., invariant di-particle masses) extraction to the clustering workflow. The aim is to mitigate the chance that BSM physics is missed because of the use of suboptimal high-level variables. 
  \end{itemize}
  \vspace{1in}
\item Prepare the ATLAS detector simulation to produce high-statistics SM background simulations for HL-LHC BSM searches.
  \begin{itemize}
  \item Develop an ML-correction algorithm on detector geometries with incrementally increasing complexity.
  \item Validate the ML-correction algorithm and integrate it into the ATLAS simulation workflow.
  \end{itemize}
\end{itemize}

\printbibliography
\clearpage
\includepdf[landscape=true]{Collaborator.pdf}
% \subsection*{Collaborators and Co-editors}
% Michael Begel (Brookhaven National Laboratory), Tim Cohen (University of Oregon), Davide Costanzo (University of Sheffield), Yuji Enari (Tokyo ICEPP), Hal Evans (Indiana University), Laura Jeanty (Univeristy of Oregon), Sabine Lammers (Indiana University), Zach Marshall (Lawrence Berkeley National Laboratory), Federico Meloni (DESY), David Miller (University of Chicago), George Redlinger (Brookhaven National Laboratory), Frederik Ruehr (Freiburg), Rosa Simoniello (CERN), Pavol Strizenec (Kosice), David Strom (University of Oregon), Dan Tovey (University of Sheffield), Guillaume Unal (CERN)

% \subsection*{Graduate and Postdoctoral Advisors and Advisees}
% Graduate advisor: Julia Thom-Levy, Cornell University
% \\Principal Postdoctoral sponsor: Stephanie Majewski, University of Oregon\\
% Graduate advisees: J. Bonilla (University of California at Davis), I. Snyder (formerly at University of Oregon now in industry), G. Gledhill (University of Oregon)


\end{document}
